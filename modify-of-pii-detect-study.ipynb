{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af07992",
   "metadata": {
    "papermill": {
     "duration": 0.006694,
     "end_time": "2024-01-20T08:01:04.827024",
     "exception": false,
     "start_time": "2024-01-20T08:01:04.820330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### This notebook is modified from <a href=\"https://www.kaggle.com/code/pjmathematician/pii-eda-presidio-baseline\">PII EDA Presidio Baseline</a> and <a href=\"https://www.kaggle.com/code/yunsuxiaozi/pii-detect-study-notebook\">PII detect study notebook</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e280a6d4",
   "metadata": {
    "papermill": {
     "duration": 0.005779,
     "end_time": "2024-01-20T08:01:04.839227",
     "exception": false,
     "start_time": "2024-01-20T08:01:04.833448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modifications \n",
    "\n",
    "#### I add my own address_recognizer and email_recognizer, URL_recognizer, and add a black list to filter potential public urls and date checker to filter noisy phone numbers. I also added Chinese note for my modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fda9ddb",
   "metadata": {
    "papermill": {
     "duration": 0.005871,
     "end_time": "2024-01-20T08:01:04.851305",
     "exception": false,
     "start_time": "2024-01-20T08:01:04.845434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Install presidio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fdcc66",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-20T08:01:04.867011Z",
     "iopub.status.busy": "2024-01-20T08:01:04.866119Z",
     "iopub.status.idle": "2024-01-20T08:01:27.803421Z",
     "shell.execute_reply": "2024-01-20T08:01:27.802096Z"
    },
    "papermill": {
     "duration": 22.948685,
     "end_time": "2024-01-20T08:01:27.806333",
     "exception": false,
     "start_time": "2024-01-20T08:01:04.857648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#安装python库 presidio_analyzer 不从python库里下载,而是从给定的链接处下载,更新到最新版本,并减少输出信息.\n",
    "!pip install -U -q presidio_analyzer --no-index --find-links=file:///kaggle/input/presidio-wheels/presidio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757713bb",
   "metadata": {
    "papermill": {
     "duration": 0.005872,
     "end_time": "2024-01-20T08:01:27.818537",
     "exception": false,
     "start_time": "2024-01-20T08:01:27.812665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import  necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c293fd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T08:01:27.832716Z",
     "iopub.status.busy": "2024-01-20T08:01:27.832249Z",
     "iopub.status.idle": "2024-01-20T08:01:34.309685Z",
     "shell.execute_reply": "2024-01-20T08:01:34.308598Z"
    },
    "papermill": {
     "duration": 6.488056,
     "end_time": "2024-01-20T08:01:34.312672",
     "exception": false,
     "start_time": "2024-01-20T08:01:27.824616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json#用于处理json格式数据的库\n",
    "import pandas as pd#导入csv文件的库\n",
    "#Presidio 是一个开源的文本分析库,用于提取文本的敏感信息.\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer, EntityRecognizer, Pattern, RecognizerResult\n",
    "from presidio_analyzer.recognizer_registry import RecognizerRegistry\n",
    "from presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NlpArtifacts\n",
    "from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n",
    "\n",
    "from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n",
    "from presidio_analyzer.predefined_recognizers import PhoneRecognizer\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c66bd",
   "metadata": {
    "papermill": {
     "duration": 0.005984,
     "end_time": "2024-01-20T08:01:34.324944",
     "exception": false,
     "start_time": "2024-01-20T08:01:34.318960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5629133d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T08:01:34.339304Z",
     "iopub.status.busy": "2024-01-20T08:01:34.338665Z",
     "iopub.status.idle": "2024-01-20T08:01:37.352232Z",
     "shell.execute_reply": "2024-01-20T08:01:37.351113Z"
    },
    "papermill": {
     "duration": 3.023364,
     "end_time": "2024-01-20T08:01:37.354720",
     "exception": false,
     "start_time": "2024-01-20T08:01:34.331356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_df):6807,train_df[0].keys():dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])\n",
      "--------------------------------------------------\n",
      "labels:{'B-STREET_ADDRESS', 'I-URL_PERSONAL', 'B-PHONE_NUM', 'B-URL_PERSONAL', 'I-NAME_STUDENT', 'B-USERNAME', 'I-ID_NUM', 'O', 'B-EMAIL', 'I-PHONE_NUM', 'B-NAME_STUDENT', 'I-STREET_ADDRESS', 'B-ID_NUM'}\n"
     ]
    }
   ],
   "source": [
    "train_df=json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"))\n",
    "print(f\"len(train_df):{len(train_df)},train_df[0].keys():{train_df[0].keys()}\")\n",
    "print(\"-\"*50)\n",
    "labels=set()\n",
    "for i in range(len(train_df)):\n",
    "    labels.update(train_df[i]['labels'])\n",
    "print(f\"labels:{labels}\")\n",
    "test_df = json.load(open('/kaggle/input/pii-detection-removal-from-educational-data/test.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2075ca",
   "metadata": {
    "papermill": {
     "duration": 0.006007,
     "end_time": "2024-01-20T08:01:37.367011",
     "exception": false,
     "start_time": "2024-01-20T08:01:37.361004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### create Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35142f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T08:01:37.381487Z",
     "iopub.status.busy": "2024-01-20T08:01:37.380826Z",
     "iopub.status.idle": "2024-01-20T08:01:43.202379Z",
     "shell.execute_reply": "2024-01-20T08:01:43.201343Z"
    },
    "papermill": {
     "duration": 5.832567,
     "end_time": "2024-01-20T08:01:43.205632",
     "exception": false,
     "start_time": "2024-01-20T08:01:37.373065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# analyzer = AnalyzerEngine()#创建文本分析器\n",
    "configuration = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n",
    "}\n",
    "\n",
    "# Create NLP engine based on configuration\n",
    "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
    "nlp_engine = provider.create_engine()\n",
    "\n",
    "# create address recognizer  创建地址分析器\n",
    "address_regex = r'\\b\\d+\\s+\\w+(\\s+\\w+)*\\s+((st(\\.)?)|(ave(\\.)?)|(rd(\\.)?)|(blvd(\\.)?)|(ln(\\.)?)|(ct(\\.)?)|(dr(\\.)?))\\b'\n",
    "address_pattern = Pattern(name=\"address\", regex=address_regex, score=0.5)\n",
    "address_recognizer = PatternRecognizer(supported_entity=\"ADDRESS_CUSTOM\", patterns = [address_pattern], context=[\"st\", \"Apt\"])\n",
    "\n",
    "# create address recognizer  创建邮箱分析器 \n",
    "email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "email_pattern = Pattern(name=\"email address\", regex=email_regex, score=0.5)\n",
    "email_recognizer = PatternRecognizer(supported_entity=\"EMAIL_CUSTOM\", patterns = [email_pattern])\n",
    "\n",
    "# create url recognizer  创建URL分析器 \n",
    "url_regex = r'https?://\\S+|www\\.\\S+'\n",
    "url_pattern = Pattern(name=\"url\", regex=url_regex, score=0.5)\n",
    "url_recognizer = PatternRecognizer(supported_entity=\"URL_CUSTOM\", patterns = [url_pattern])\n",
    "\n",
    "# create phone recognizer  创建电话分析器 \n",
    "phone_recognizer = PhoneRecognizer(context=['phone', 'number', 'telephone', 'cell', 'cellphone', 'mobile', 'call', 'ph', 'tel', 'mobile', 'Email'])\n",
    "\n",
    "\n",
    "registry = RecognizerRegistry()\n",
    "registry.load_predefined_recognizers()\n",
    "registry.add_recognizer(address_recognizer)\n",
    "registry.add_recognizer(email_recognizer)\n",
    "registry.add_recognizer(url_recognizer)\n",
    "registry.add_recognizer(phone_recognizer)\n",
    "\n",
    "\n",
    "# Pass the created NLP engine and supported_languages to the AnalyzerEngine\n",
    "analyzer = AnalyzerEngine(\n",
    "    nlp_engine=nlp_engine, \n",
    "    supported_languages=[\"en\"],\n",
    "    registry=registry,\n",
    "    context_aware_enhancer=\n",
    "        LemmaContextAwareEnhancer(context_similarity_factor=0.8, min_score_with_context_similarity=0.4)\n",
    ")\n",
    "\n",
    "# remove date info in phone number recognizer  移除日期类型的电话号码\n",
    "def is_valid_date(text):\n",
    "    try:\n",
    "        # Attempt to parse the text as a date\n",
    "        parsed_date = parser.parse(text)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba370f9b",
   "metadata": {
    "papermill": {
     "duration": 0.008432,
     "end_time": "2024-01-20T08:01:43.223223",
     "exception": false,
     "start_time": "2024-01-20T08:01:43.214791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7ecbeb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T08:01:43.242359Z",
     "iopub.status.busy": "2024-01-20T08:01:43.241963Z",
     "iopub.status.idle": "2024-01-20T08:01:43.252334Z",
     "shell.execute_reply": "2024-01-20T08:01:43.251045Z"
    },
    "papermill": {
     "duration": 0.022993,
     "end_time": "2024-01-20T08:01:43.254912",
     "exception": false,
     "start_time": "2024-01-20T08:01:43.231919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#对文本进行分词成下标,也就是每个词的起始位置和终止位置\n",
    "def tokens2index(row):#传入一个json解析的数据\n",
    "    tokens  = row['tokens']#分词的数据['apple','bool','cat',……]\n",
    "    start_ind = []\n",
    "    end_ind = []\n",
    "    prev_ind = 0\n",
    "    for tok in tokens:#取出一个词\n",
    "        #比如现在的位置是30,从30开始往后找index为5,那么起始位置就是35\n",
    "        start = prev_ind + row['full_text'][prev_ind:].index(tok)\n",
    "        end = start+len(tok)#起始位置+词的长度=终点位置\n",
    "        #储存这个词的起点和终点位置\n",
    "        start_ind.append(start)\n",
    "        end_ind.append(end)\n",
    "        prev_ind = end\n",
    "    return start_ind, end_ind#返回的是分词后每个词的起始位置和终点位置\n",
    "\n",
    "#二分查找,找到arr[index]=target\n",
    "def find_or_next_larger(arr, target):#arr:分词后每个词的start,target:一个实体的start\n",
    "    left, right = 0, len(arr) - 1#arr的最左边和最右边\n",
    "\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "\n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return left\n",
    "def count_trailing_whitespaces(word):\n",
    "    #单词的长度-单词去掉尾部空格后的长度=单词尾部的长度\n",
    "    return len(word) - len(word.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5a391",
   "metadata": {
    "papermill": {
     "duration": 0.006031,
     "end_time": "2024-01-20T08:01:43.269482",
     "exception": false,
     "start_time": "2024-01-20T08:01:43.263451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b7895c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T08:01:43.284049Z",
     "iopub.status.busy": "2024-01-20T08:01:43.283651Z",
     "iopub.status.idle": "2024-01-20T08:01:43.288813Z",
     "shell.execute_reply": "2024-01-20T08:01:43.287930Z"
    },
    "papermill": {
     "duration": 0.015137,
     "end_time": "2024-01-20T08:01:43.290945",
     "exception": false,
     "start_time": "2024-01-20T08:01:43.275808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add URL black list 创建URL黑名单\n",
    "black_list = [\"wikipedia\", \"coursera\", \".pdf\", \".PDF\", \"article\", \".png\", \".gov\", \".work\", \".ai\", \".firm\", \".arts\", \".store\", \".rec\", \".biz\", \".travel\" ]\n",
    "white_list = ['phone', 'number', 'telephone', 'cell', 'cellphone', 'mobile', 'call', 'ph', 'tel', 'mobile', 'Email']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "481d61d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T08:01:43.305767Z",
     "iopub.status.busy": "2024-01-20T08:01:43.305097Z",
     "iopub.status.idle": "2024-01-20T08:01:45.525847Z",
     "shell.execute_reply": "2024-01-20T08:01:45.524567Z"
    },
    "papermill": {
     "duration": 2.231997,
     "end_time": "2024-01-20T08:01:45.529149",
     "exception": false,
     "start_time": "2024-01-20T08:01:43.297152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tokens2index: 100%|██████████| 10/10 [00:00<00:00, 789.40it/s]\n",
      "Analyzing entities: 100%|██████████| 10/10 [00:02<00:00,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "df_ = test_df #test_df #train_df\n",
    "PHONE_NUM, NAME_STUDENT, URL_PERSONAL, EMAIL, STREET_ADDRESS, ID_NUM, USERNAME = [],[],[],[],[],[], []\n",
    "\n",
    "preds = []\n",
    "#查找每个词分词后的起始位置和终点位置\n",
    "for i in tqdm(range(len(df_)), desc=\"Processing tokens2index\"):\n",
    "    start, end = tokens2index(df_[i])\n",
    "    #将每个词分词后的起始位置和终点位置加入json文件里.\n",
    "    df_[i]['start'] = start\n",
    "    df_[i]['end'] = end\n",
    "    \n",
    "for i, d in tqdm(enumerate(df_), total=len(df_), desc=\"Analyzing entities\"):#取出d=df_[i]\n",
    "    #传入的文本是full_text,对英文文本进行分析,需要识别的是电话号码,人,url和email这几种类型.\n",
    "    #results:[type: PERSON, start: 22, end: 37, score: 0.85]\n",
    "    results = analyzer.analyze(text=d['full_text'],\n",
    "                           entities=[\"PHONE_NUMBER\", \n",
    "                                     \"PERSON\", \n",
    "                                     \"URL_CUSTOM\", #\"IP_ADDRESS\", #\"URL\",\n",
    "                                     \"EMAIL_ADDRESS\", \"EMAIL_CUSTOM\", \n",
    "                                     \"ADDRESS_CUSTOM\",\n",
    "                                     \"US_SSN\", \"US_ITIN\", \"US_PASSPORT\", \"US_BANK_NUMBER\",\n",
    "                                     \"USERNAME\"],\n",
    "                           language='en',\n",
    "#                            score_threshold=0.04,\n",
    "                            )\n",
    "    pre_preds = []\n",
    "    for r in results:#遍历找到过的每个实体,r:[type: PERSON, start: 22, end: 37, score: 0.85]\n",
    "        #就是第s个词就是某个实体的开始\n",
    "        s = find_or_next_larger(d['start'], r.start)#d['start'][s]=r.start\n",
    "        end = r.end#实体终点\n",
    "        word = d['full_text'][r.start:r.end]#文本里找单词\n",
    "        end = end - count_trailing_whitespaces(word)#end减去尾部的空格就是单词自身尾部的下标\n",
    "        temp_preds = [s]#实体单词的集合从第s个单词开始,然后连续几个单词?\n",
    "        try:\n",
    "            #实体可能不是一个单词,分词的下一个单词如果还没有到达实体的尾部,就把下一个单词加上\n",
    "            while d['end'][s+1] <= end:\n",
    "                temp_preds.append(s+1)\n",
    "                s +=1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #找出来的实体是什么,我们就给它打对应的标签\n",
    "        tmp = False\n",
    "        \n",
    "        if r.entity_type == 'USERNAME':\n",
    "            label =  'USERNAME'\n",
    "            USERNAME.append(d['full_text'][r.start:r.end])\n",
    "            \n",
    "        if r.entity_type == 'PHONE_NUMBER':\n",
    "            #检查是不是日期类型\n",
    "            if is_valid_date(word):\n",
    "                continue\n",
    "            for w in white_list:\n",
    "                if w in d['full_text'][max(r.start-50, 0):min(r.end+50, len(d['full_text']))]:\n",
    "                    tmp = False\n",
    "                    break\n",
    "                else:\n",
    "                    tmp = True \n",
    "                    \n",
    "            label =  'PHONE_NUM'\n",
    "            PHONE_NUM.append(d['full_text'][r.start:r.end])\n",
    "            \n",
    "        if r.entity_type == 'PERSON':\n",
    "            label =  'NAME_STUDENT'\n",
    "            NAME_STUDENT.append(d['full_text'][r.start:r.end])\n",
    "            \n",
    "        if r.entity_type == 'ADDRESS_CUSTOM':\n",
    "            label = 'STREET_ADDRESS'\n",
    "            STREET_ADDRESS.append(d['full_text'][r.start:r.end])\n",
    "            \n",
    "        if r.entity_type == 'US_SSN' or r.entity_type == 'US_ITIN' or r.entity_type == 'US_PASSPORT' or r.entity_type == 'US_BANK_NUMBER':\n",
    "            label = 'ID_NUM'\n",
    "            ID_NUM.append(d['full_text'][r.start:r.end])\n",
    "            \n",
    "        if r.entity_type == 'EMAIL_ADDRESS' or r.entity_type == 'EMAIL_CUSTOM':\n",
    "            label = 'EMAIL'\n",
    "            EMAIL.append(d['full_text'][r.start:r.end])\n",
    "            \n",
    "        if r.entity_type == 'URL_CUSTOM':# or r.entity_type == 'IP_ADDRESS' or \"http\" in word:\n",
    "            #去除掉黑名单里的标签\n",
    "            for w in black_list:\n",
    "                if w in word:\n",
    "                    tmp = True\n",
    "                    break\n",
    "            \n",
    "            label = 'URL_PERSONAL'\n",
    "            URL_PERSONAL.append(d['full_text'][r.start:r.end])\n",
    "            \n",
    "        if tmp:\n",
    "            continue\n",
    "        \n",
    "            \n",
    "        #取出实体中的一个分词的下标\n",
    "        for p in temp_preds:\n",
    "            if len(pre_preds) > 0:#第2次及以后经过这里.\n",
    "                \"\"\"\n",
    "                新开始一个r的时候,pre_preds[-1]['rlabel']还是上一个实体的r.entity_type\n",
    "                此时也许会不等于这个实体的r.entity_type,换句话说,第一个等号就是还在同一个实体里.\n",
    "                p - pre_preds[-1]['token']==1就是连续的意思\n",
    "                \"\"\"\n",
    "                if pre_preds[-1]['rlabel'] == r.entity_type and (p - pre_preds[-1]['token']==1):\n",
    "                    label_f = \"I-\"+label#实体的中间位置\n",
    "                else:\n",
    "                    label_f = \"B-\"+label#否则就是下一个实体的开始\n",
    "            else:#第一个label是起始位置,故标记为‘B-’\n",
    "                label_f = \"B-\"+label\n",
    "            #保存document,从第p个单词开始,标签为label_f\n",
    "            pre_preds.append(({\n",
    "                    \"document\":d['document'],\n",
    "                    \"token\":p,\n",
    "                    \"label\":label_f,\n",
    "                    \"rlabel\":r.entity_type#实体的类型\n",
    "                }))\n",
    "    preds.extend(pre_preds)#遍历完这个数据之后,将所有找到的实体做汇总"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88301f11",
   "metadata": {
    "papermill": {
     "duration": 0.007309,
     "end_time": "2024-01-20T08:01:45.544434",
     "exception": false,
     "start_time": "2024-01-20T08:01:45.537125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83ec9dec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T08:01:45.560905Z",
     "iopub.status.busy": "2024-01-20T08:01:45.560483Z",
     "iopub.status.idle": "2024-01-20T08:01:45.598048Z",
     "shell.execute_reply": "2024-01-20T08:01:45.596936Z"
    },
    "papermill": {
     "duration": 0.048684,
     "end_time": "2024-01-20T08:01:45.600523",
     "exception": false,
     "start_time": "2024-01-20T08:01:45.551839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>52</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>55</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  document  token           label\n",
       "0       0         7      9  B-NAME_STUDENT\n",
       "1       1         7     10  I-NAME_STUDENT\n",
       "2       2         7     52  B-NAME_STUDENT\n",
       "3       3         7     53  I-NAME_STUDENT\n",
       "4       4         7     55  B-NAME_STUDENT"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#得到预测结果后,最后一行r.entity_type不要,reset_index\n",
    "submission = pd.DataFrame(preds).iloc[:,:-1].reset_index()\n",
    "#index变成row_id,剩下3列就是submission的列名\n",
    "submission.columns = ['row_id','document', 'token', 'label']\n",
    "#保存csv文件\n",
    "submission.to_csv('submission.csv', index = False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69e4e1",
   "metadata": {
    "papermill": {
     "duration": 0.007535,
     "end_time": "2024-01-20T08:01:45.616098",
     "exception": false,
     "start_time": "2024-01-20T08:01:45.608563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "sourceId": 159367535,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 47.089214,
   "end_time": "2024-01-20T08:01:47.152152",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-20T08:01:00.062938",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
