{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7429898,"sourceType":"datasetVersion","datasetId":4319117}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformer NER baseline [lb 0.854]\n\nThe following is a basic script to train and run inference using `transformers` using 2x T4 GPUs. You might get better performance if you use a bigger model, or one that has already been trained on NER.\n\nIt includes processing to correctly map the given tokens with labels during training and vice versa when running inference.\n\n\nUpdate: Thanks to @takanashihumbert, I switched `tokens` for `token_map` which helped improve the score from 0.569 to 0.854!\n\n\ndeberta-v3-small: lb 0.576  \ndeberta-v3-base: lb 0.569 (before update) --> 0.854 (after update) ","metadata":{}},{"cell_type":"code","source":"TRAINING = False # be sure to turn internet off if doing inference\n\nTRAINING_MODEL_PATH = \"microsoft/deberta-v3-large\"\nTRAINING_MAX_LENGTH = 512\n\nINFERENCE_MODEL_PATH = \"/kaggle/input/pii-data-detection-baseline/output/checkpoint-240\"\nINFERENCE_MAX_LENGTH = 2000\n\nif TRAINING:\n    !pip install seqeval evaluate -q\n    !pip install -U datasets accelerate transformers -q","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:37:35.577723Z","iopub.execute_input":"2024-01-25T21:37:35.578114Z","iopub.status.idle":"2024-01-25T21:37:35.587366Z","shell.execute_reply.started":"2024-01-25T21:37:35.578083Z","shell.execute_reply":"2024-01-25T21:37:35.585800Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"if TRAINING:\n    import json\n\n    data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"))\n\n    print(len(data))\n    print(data[0].keys())\n\n    x = data[0]\n\n    print(x[\"tokens\"][:10])\n    print(x[\"labels\"][:10])\n    print(x[\"trailing_whitespace\"][:10])","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:37:35.589758Z","iopub.execute_input":"2024-01-25T21:37:35.590432Z","iopub.status.idle":"2024-01-25T21:37:35.604681Z","shell.execute_reply.started":"2024-01-25T21:37:35.590393Z","shell.execute_reply":"2024-01-25T21:37:35.603633Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"if TRAINING:\n    from itertools import chain\n\n    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n    label2id = {l: i for i,l in enumerate(all_labels)}\n    id2label = {v:k for k,v in label2id.items()}\n\n    id2label","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:37:35.605889Z","iopub.execute_input":"2024-01-25T21:37:35.606208Z","iopub.status.idle":"2024-01-25T21:37:35.621640Z","shell.execute_reply.started":"2024-01-25T21:37:35.606175Z","shell.execute_reply":"2024-01-25T21:37:35.620746Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"if TRAINING:\n    from transformers import AutoTokenizer\n    import numpy as np\n\n    tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)\n\n    def tokenize(example, tokenizer, label2id):\n        text = []\n\n        # these are at the character level\n        labels = []\n\n        for t, l, ws in zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]):\n\n            text.append(t)\n            labels.extend([l]*len(t))\n\n            # if there is trailing whitespace\n            if ws:\n                text.append(\" \")\n                labels.append(\"O\")\n\n\n        tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=False)\n\n        labels = np.array(labels)\n\n        text = \"\".join(text)\n        token_labels = []\n\n        for start_idx, end_idx in tokenized.offset_mapping:\n\n            # CLS token\n            if start_idx + end_idx == 0: \n                token_labels.append(label2id[\"O\"])\n                continue\n\n            # case when token starts with whitespace\n            if text[start_idx].isspace():\n                start_idx += 1\n            \n            while start_idx >= len(labels):\n                start_idx -= 1\n\n            token_labels.append(label2id[labels[start_idx]])\n\n        length = len(tokenized.input_ids)\n\n        return {\n            **tokenized,\n            \"labels\": token_labels,\n            \"length\": length\n        }","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:37:35.623238Z","iopub.execute_input":"2024-01-25T21:37:35.623598Z","iopub.status.idle":"2024-01-25T21:37:35.635911Z","shell.execute_reply.started":"2024-01-25T21:37:35.623565Z","shell.execute_reply":"2024-01-25T21:37:35.635016Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"if TRAINING:\n    from datasets import Dataset\n\n    ds = Dataset.from_dict({\n        \"full_text\": [x[\"full_text\"] for x in data],\n        \"document\": [x[\"document\"] for x in data],\n        \"tokens\": [x[\"tokens\"] for x in data],\n        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n        \"provided_labels\": [x[\"labels\"] for x in data],\n    })","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:37:35.641152Z","iopub.execute_input":"2024-01-25T21:37:35.641436Z","iopub.status.idle":"2024-01-25T21:37:35.655238Z","shell.execute_reply.started":"2024-01-25T21:37:35.641411Z","shell.execute_reply":"2024-01-25T21:37:35.654323Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"if TRAINING:\n    ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id}, num_proc=2)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:37:35.656176Z","iopub.execute_input":"2024-01-25T21:37:35.656475Z","iopub.status.idle":"2024-01-25T21:37:35.666737Z","shell.execute_reply.started":"2024-01-25T21:37:35.656451Z","shell.execute_reply":"2024-01-25T21:37:35.665817Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"if TRAINING:\n    # Confirm that alignment is good\n\n    # run multiple times to see different rows\n    x = ds.shuffle()[0]\n\n    for t,l in zip(x[\"tokens\"], x[\"provided_labels\"]):\n        if l != \"O\":\n            print((t,l))\n\n    print(\"*\"*100)\n            \n    for t, l in zip(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]), x[\"labels\"]):\n        if id2label[l] != \"O\":\n            print((t,id2label[l]))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:37:35.667908Z","iopub.execute_input":"2024-01-25T21:37:35.668178Z","iopub.status.idle":"2024-01-25T21:37:35.678610Z","shell.execute_reply.started":"2024-01-25T21:37:35.668154Z","shell.execute_reply":"2024-01-25T21:37:35.677603Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## There are some long ones that will get truncated when training if you use a typical max_length\n\nThere might be some key labels that are at the end that are being missed.","metadata":{}},{"cell_type":"code","source":"if TRAINING:\n    import matplotlib.pyplot as plt\n\n\n    plt.hist(ds[\"length\"], bins=100);","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:37:35.679887Z","iopub.execute_input":"2024-01-25T21:37:35.680254Z","iopub.status.idle":"2024-01-25T21:37:35.693285Z","shell.execute_reply.started":"2024-01-25T21:37:35.680222Z","shell.execute_reply":"2024-01-25T21:37:35.692336Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"if TRAINING:\n    import pandas as pd\n    import plotly.express as px\n    from collections import Counter\n    \n    \n    group = []\n    labels = []\n    \n    group_thresholds = [0, 50, 100, 200, 500, 1000, 2000, 10000]\n    \n    for sample_labels in ds[\"provided_labels\"]:\n        for i, label in enumerate(sample_labels):\n            if label != \"O\":\n                for j in range(1, len(group_thresholds)):\n                    lower = group_thresholds[j-1]\n                    upper = group_thresholds[j]\n                    \n                    if lower <= i < upper:\n                        group.append(f\"{lower}-{upper}\")\n                        labels.append(label)\n                        break\n    \n    pairs = list(zip(labels, group))\n    \n    counts = Counter(pairs)\n    \n    \n    data = {\n        \"label\": [],\n        \"count\": [],\n        \"range\": [],\n    }\n    \n    for (label, range_), count in counts.items():\n        data[\"label\"].append(label)\n        data[\"range\"].append(range_)\n        data[\"count\"].append(count)\n    \n                \n    df = pd.DataFrame(data)\n    \n    \n    px.scatter(df, x=\"range\", y=\"count\", color=\"label\", log_y=True, height=1000)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:37:35.696483Z","iopub.execute_input":"2024-01-25T21:37:35.696799Z","iopub.status.idle":"2024-01-25T21:37:35.707229Z","shell.execute_reply.started":"2024-01-25T21:37:35.696758Z","shell.execute_reply":"2024-01-25T21:37:35.706271Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"%%writefile run.py\n\nimport os\nimport json\nimport argparse\nimport random\nfrom itertools import chain\nfrom functools import partial\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom tokenizers import AddedToken\nimport evaluate\nfrom datasets import Dataset\nimport numpy as np\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/468844\ndef filter_no_pii(example, percent_allow=0.2):\n    # Return True if there is PII\n    # Or 20% of the time if there isn't\n    \n    has_pii = set(\"O\") != set(example[\"provided_labels\"])\n    \n    return has_pii or (random.random() < percent_allow)\n\ndef tokenize(example, tokenizer, label2id, max_length):\n    text = []\n    labels = []\n    \n    for t, l, ws in zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]):\n        \n        text.append(t)\n        labels.extend([l]*len(t))\n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n    \n    \n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n    \n    labels = np.array(labels)\n    \n    text = \"\".join(text)\n    token_labels = []\n    \n    for start_idx, end_idx in tokenized.offset_mapping:\n        \n        # CLS token\n        if start_idx == 0 and end_idx == 0: \n            token_labels.append(label2id[\"O\"])\n            continue\n        \n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n        \n        while start_idx >= len(labels):\n            start_idx -= 1\n            \n        token_labels.append(label2id[labels[start_idx]])\n        \n    length = len(tokenized.input_ids)\n        \n    return {\n        **tokenized,\n        \"labels\": token_labels,\n        \"length\": length\n    }\n    \ndef compute_metrics(p, metric, all_labels):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n\n    # Unpack nested dictionaries\n    final_results = {}\n    for key, value in results.items():\n        if isinstance(value, dict):\n            for n, v in value.items():\n                final_results[f\"{key}_{n}\"] = v\n        else:\n            final_results[key] = value\n    return final_results   \n\ndef main():\t20\n    \n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\"--model_path\", type=str)\n    parser.add_argument(\"--max_length\", type=int)\n    \n    args = parser.parse_args()\n    \n    \n    data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"))\n\n\n    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n    label2id = {l: i for i,l in enumerate(all_labels)}\n    id2label = {v:k for k,v in label2id.items()}\n\n\n    ds = Dataset.from_dict({\n        \"full_text\": [x[\"full_text\"] for x in data],\n        \"document\": [x[\"document\"] for x in data],\n        \"tokens\": [x[\"tokens\"] for x in data],\n        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n        \"provided_labels\": [x[\"labels\"] for x in data],b 0.576  \n    })\n\n    \n    tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n    \n    # lots of newlines in the text\n    # adding this should be helpful\n    tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n    \n    ds = ds.filter(\n        filter_no_pii,\n        num_proc=2,\n    )\n    \n    ds = ds.map(\n        tokenize, \n        fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": args.max_length}, \n        num_proc=2,\n    )\n\n\n    metric = evaluate.load(\"seqeval\")\n\n\n    model = AutoModelForTokenClassification.from_pretrained(args.model_path, num_labels=len(all_labels), id2label=id2label, label2id=label2id)\n    model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=16)\n\n    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n\n    args = TrainingArguments(\n        \"output\", \n        fp16=True, \n        learning_rate=5e-5,\n        weight_decay=0.01,\n        warmup_ratio=0.1,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=4, \n        report_to=\"none\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        save_total_limit=1,\n        logging_steps=5,\n        metric_for_best_model=\"overall_recall\",\n        greater_is_better=True,\n        gradient_checkpointing=True,\n        num_train_epochs=1\n        dataloader_num_workers=1,\n    )\n\n    # may want to try to balance classes in splits\n    final_ds = ds.train_test_split(test_size=0.2)\n\n\n    trainer = Trainer(\n        model=model, \n        args=args, \n        train_dataset=final_ds[\"train\"], \n        eval_dataset=final_ds[\"test\"], \n        data_collator=collator, \n        tokenizer=tokenizer,\n        compute_metrics=partial(compute_metrics, metric=metric, all_labels=all_labels),\n    )\n\n\n    trainer.train()\n    \n    \nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:37:35.708576Z","iopub.execute_input":"2024-01-25T21:37:35.708858Z","iopub.status.idle":"2024-01-25T21:37:35.721935Z","shell.execute_reply.started":"2024-01-25T21:37:35.708834Z","shell.execute_reply":"2024-01-25T21:37:35.720959Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Overwriting run.py\n","output_type":"stream"}]},{"cell_type":"code","source":"if TRAINING:\n    # utilize both t4 gpus\n    !accelerate launch --multi_gpu --num_processes 2 run.py \\\n      --model_path $TRAINING_MODEL_PATH \\\n      --max_length $TRAINING_MAX_LENGTH","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:37:35.723047Z","iopub.execute_input":"2024-01-25T21:37:35.723745Z","iopub.status.idle":"2024-01-25T21:37:35.735770Z","shell.execute_reply.started":"2024-01-25T21:37:35.723707Z","shell.execute_reply":"2024-01-25T21:37:35.734814Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"%%writefile infer.py\n\nimport json\nimport argparse\nfrom itertools import chain\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom datasets import Dataset\nimport numpy as np\n\ndef tokenize(example, tokenizer, max_length):\n    text = []\n    token_map = []\n    \n    idx = 0\n    \n    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        \n        text.append(t)\n        token_map.extend([idx]*len(t))\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        idx += 1\n            \n        \n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n    \n        \n    return {\n        **tokenized,\n        \"token_map\": token_map,\n    }\n\ndef main():\n    \n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\"--model_path\", type=str)\n    parser.add_argument(\"--max_length\", type=int)\n    \n    args = parser.parse_args()\n    \n    data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n    \n    ds = Dataset.from_dict({\n        \"full_text\": [x[\"full_text\"] for x in data],\n        \"document\": [x[\"document\"] for x in data],\n        \"tokens\": [x[\"tokens\"] for x in data],\n        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n    })\n\n    \n    tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n    ds = ds.map(\n        tokenize, \n        fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": args.max_length}, \n        num_proc=2,\n    )\n    \n    model = AutoModelForTokenClassification.from_pretrained(args.model_path)\n\n    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n\n    args = TrainingArguments(\n        \".\", \n        per_device_eval_batch_size=4, \n        report_to=\"none\",\n    )\n    \n    trainer = Trainer(\n        model=model, \n        args=args, \n        data_collator=collator, \n        tokenizer=tokenizer,\n    )\n    \n    \n    predictions = trainer.predict(ds).predictions\n\n    ds.to_parquet(\"test_ds.pq\")\n    \n    np.save(\"preds.npy\", predictions)\n    \n    \nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:37:35.737658Z","iopub.execute_input":"2024-01-25T21:37:35.737914Z","iopub.status.idle":"2024-01-25T21:37:35.749405Z","shell.execute_reply.started":"2024-01-25T21:37:35.737891Z","shell.execute_reply":"2024-01-25T21:37:35.748491Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Overwriting infer.py\n","output_type":"stream"}]},{"cell_type":"code","source":"if not TRAINING:\n    \n    !accelerate launch --num_processes 2 infer.py \\\n      --model_path $INFERENCE_MODEL_PATH \\\n      --max_length $INFERENCE_MAX_LENGTH","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:37:35.750623Z","iopub.execute_input":"2024-01-25T21:37:35.750940Z","iopub.status.idle":"2024-01-25T21:38:23.004134Z","shell.execute_reply.started":"2024-01-25T21:37:35.750909Z","shell.execute_reply":"2024-01-25T21:38:23.003086Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nMap (num_proc=2):   0%|                           | 0/10 [00:00<?, ? examples/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nMap (num_proc=2):  50%|█████████▌         | 5/10 [00:00<00:00,  8.90 examples/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nMap (num_proc=2): 100%|██████████████████| 10/10 [00:00<00:00, 12.25 examples/s]\nMap (num_proc=2): 100%|██████████████████| 10/10 [00:00<00:00, 11.80 examples/s]\n100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.25it/s]\nCreating parquet from Arrow format: 100%|█████████| 1/1 [00:00<00:00, 42.73ba/s]\nCreating parquet from Arrow format: 100%|█████████| 1/1 [00:00<00:00, 42.83ba/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Recall is much more important than precision, so it might make sense to make predictions even if they aren't the highest score","metadata":{}},{"cell_type":"code","source":"if not TRAINING:\n    \n    import numpy as np\n    import json\n    from datasets import Dataset\n    import pandas as pd\n    from pathlib import Path\n\n    config = json.load(open(Path(INFERENCE_MODEL_PATH) / \"config.json\"))\n\n    id2label = config[\"id2label\"]\n\n    preds = np.load(\"preds.npy\")\n\n    ds = Dataset.from_parquet(\"test_ds.pq\")\n\n    preds = preds.argmax(-1)\n\n    triplets = []\n    document, token, label, token_str = [], [], [], []\n    for p, token_map, offsets, tokens, doc in zip(preds, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[str(token_pred)]\n\n            if start_idx + end_idx == 0: continue\n\n            if token_map[start_idx] == -1: \n                start_idx += 1\n\n            # ignore \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n\n            if start_idx >= len(token_map): break\n\n            token_id = token_map[start_idx]\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                if triplet not in triplets:\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.append(triplet)\n\n\n    df = pd.DataFrame({\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n    df[\"row_id\"] = list(range(len(df)))\n\n    display(df.head(50))\n\n\n    df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:38:23.007780Z","iopub.execute_input":"2024-01-25T21:38:23.008142Z","iopub.status.idle":"2024-01-25T21:38:23.361032Z","shell.execute_reply.started":"2024-01-25T21:38:23.008112Z","shell.execute_reply":"2024-01-25T21:38:23.360120Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning:\n\nA NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"632f32f40ae1474185fd710c56c31444"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    document  token           label  \\\n0          7      9  B-NAME_STUDENT   \n1          7     10  I-NAME_STUDENT   \n2          7    482  B-NAME_STUDENT   \n3          7    483  I-NAME_STUDENT   \n4          7    741  B-NAME_STUDENT   \n5          7    742  I-NAME_STUDENT   \n6         10      0  B-NAME_STUDENT   \n7         10      1  I-NAME_STUDENT   \n8         10    464  B-NAME_STUDENT   \n9         10    465  I-NAME_STUDENT   \n10        16      4  B-NAME_STUDENT   \n11        16      5  I-NAME_STUDENT   \n12        20      5  B-NAME_STUDENT   \n13        20      6  I-NAME_STUDENT   \n14        56     12  B-NAME_STUDENT   \n15        56     13  I-NAME_STUDENT   \n16        86      6  B-NAME_STUDENT   \n17        86      7  I-NAME_STUDENT   \n18        93      0  B-NAME_STUDENT   \n19        93      1  I-NAME_STUDENT   \n20       104      7  B-NAME_STUDENT   \n21       104      8  B-NAME_STUDENT   \n22       104      9  I-NAME_STUDENT   \n23       112      5  B-NAME_STUDENT   \n24       112      6  I-NAME_STUDENT   \n25       123     32  B-NAME_STUDENT   \n26       123     33  I-NAME_STUDENT   \n27       123    223  B-NAME_STUDENT   \n28       123    224  I-NAME_STUDENT   \n29       123   1575  B-URL_PERSONAL   \n\n                                     token_str  row_id  \n0                                     Nathalie       0  \n1                                        Sylla       1  \n2                                     Nathalie       2  \n3                                        Sylla       3  \n4                                     Nathalie       4  \n5                                        Sylla       5  \n6                                        Diego       6  \n7                                      Estrada       7  \n8                                        Diego       8  \n9                                      Estrada       9  \n10                                    Gilberto      10  \n11                                      Gamboa      11  \n12                                       Sindy      12  \n13                                      Samaca      13  \n14                                      Nadine      14  \n15                                        Born      15  \n16                                      Eladio      16  \n17                                       Amaya      17  \n18                                      Silvia      18  \n19                                  Villalobos      19  \n20                                          Dr      20  \n21                                       Sakir      21  \n22                                       Ahmad      22  \n23                                   Francisco      23  \n24                                    Ferreira      24  \n25                                     Stefano      25  \n26                                      Lovato      26  \n27                                Gerashchenko      27  \n28                                        Igor      28  \n29  https://cyberleninka.ru/article/n/14398333      29  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>token</th>\n      <th>label</th>\n      <th>token_str</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>9</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>10</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>482</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>483</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>741</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7</td>\n      <td>742</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10</td>\n      <td>464</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>465</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>16</td>\n      <td>4</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Gilberto</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>16</td>\n      <td>5</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Gamboa</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>20</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sindy</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>20</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Samaca</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>56</td>\n      <td>12</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nadine</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>56</td>\n      <td>13</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Born</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>86</td>\n      <td>6</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Eladio</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>86</td>\n      <td>7</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Amaya</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>93</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Silvia</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>93</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Villalobos</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>104</td>\n      <td>7</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Dr</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>104</td>\n      <td>8</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sakir</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>104</td>\n      <td>9</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ahmad</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>112</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Francisco</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>112</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ferreira</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>123</td>\n      <td>32</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Stefano</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>123</td>\n      <td>33</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Lovato</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>123</td>\n      <td>223</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Gerashchenko</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>123</td>\n      <td>224</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Igor</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>123</td>\n      <td>1575</td>\n      <td>B-URL_PERSONAL</td>\n      <td>https://cyberleninka.ru/article/n/14398333</td>\n      <td>29</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}