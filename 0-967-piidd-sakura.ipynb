{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7803679,"sourceType":"datasetVersion","datasetId":4340749},{"sourceId":7908837,"sourceType":"datasetVersion","datasetId":4646023},{"sourceId":8044584,"sourceType":"datasetVersion","datasetId":4743333}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport re\n\nfrom itertools import chain\nimport pandas as pd\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom datasets import Dataset\nimport numpy as np\nimport os\nimport gc\nimport torch\nfrom scipy.special import softmax\nfrom spacy.lang.en import English","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-07T11:11:23.354843Z","iopub.execute_input":"2024-04-07T11:11:23.355178Z","iopub.status.idle":"2024-04-07T11:11:45.392154Z","shell.execute_reply.started":"2024-04-07T11:11:23.355148Z","shell.execute_reply":"2024-04-07T11:11:45.391152Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-07 11:11:34.153163: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-07 11:11:34.153266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-07 11:11:34.282626: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"nlp = English()\nINFERENCE_MAX_LENGTH = 3500\nthreshold = 0.99\nemail_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\nphone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\nurl_regex = re.compile(\n    r'http[s]?://'  # http or https\n    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'  # domain...\n    r'localhost|'  # localhost...\n    r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'  # ...or ip\n    r'(?::\\d+)?'  # optional port\n    r'(?:/?|[/?]\\S+)', re.IGNORECASE)\nstreet_regex = re.compile(r'\\d{1,4} [\\w\\s]{1,20}(?:street|apt|st|avenue|ave|road|rd|highway|hwy|square|sq|trail|trl|drive|dr|court|ct|parkway|pkwy|circle|cir|boulevard|blvd)\\W?(?=\\s|$)', re.IGNORECASE)\n\n# Initialize a tokenizer and model from the pretrained model path\nmodel_paths = {\n    '/kaggle/input/pii-deberta-models/cuerpo-de-piiranha': 2/20,\n    '/kaggle/input/pii-deberta-models/cola del piinguuino' : 1/20,\n    '/kaggle/input/pii-deberta-models/cabeza-del-piinguuino': 5/20,\n    '/kaggle/input/pii-models/piidd-org-sakura': 2/20,\n    '/kaggle/input/pii-deberta-v3-large-0406-2005': 10/20\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:11:45.393841Z","iopub.execute_input":"2024-04-07T11:11:45.394405Z","iopub.status.idle":"2024-04-07T11:11:45.679034Z","shell.execute_reply.started":"2024-04-07T11:11:45.394375Z","shell.execute_reply":"2024-04-07T11:11:45.678254Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# This be a function called 'tokenize', me hearties!\ndef tokenize(example, tokenizer):\n    # We be creatin' two empty lists, 'text' and 'token_map', to store our tokens and their respective maps.\n    text = []\n    token_map = []\n    \n    # We start the 'idx' at 0, it be used to keep track of the tokens.\n    idx = 0\n    \n    # Now, we be loopin' through the tokens and their trailin' white spaces.\n    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        \n        # We add the token 't' to the 'text' list.\n        text.append(t)\n        \n        # We be extendin' the 'token_map' list by repeatin' the 'idx' as many times as the length of token 't'.\n        token_map.extend([idx]*len(t))\n        \n        # If there be trailin' whitespace (ws), we add a space to 'text' and mark it with a '-1' in 'token_map'.\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        # We increment 'idx' to keep track of the next token.\n        idx += 1\n        \n    # Now, we tokenize the concatenated 'text' and return offsets mappings along with 'token_map'.\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH)\n    \n    # We return a dictionary containin' the tokenized data and the 'token_map'.\n    return {\n        **tokenized,\n        \"token_map\": token_map,\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:11:45.680161Z","iopub.execute_input":"2024-04-07T11:11:45.680459Z","iopub.status.idle":"2024-04-07T11:11:45.687768Z","shell.execute_reply.started":"2024-04-07T11:11:45.680434Z","shell.execute_reply":"2024-04-07T11:11:45.686887Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n    idx = 0\n    spans = []\n    span = []\n\n    for i, token in enumerate(document):\n        if token != target[idx]:\n            idx = 0\n            span = []\n            continue\n        span.append(i)\n        idx += 1\n        if idx == len(target):\n            spans.append(span)\n            span = []\n            idx = 0\n            continue\n    \n    return spans","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:11:45.689889Z","iopub.execute_input":"2024-04-07T11:11:45.690227Z","iopub.status.idle":"2024-04-07T11:11:45.702105Z","shell.execute_reply.started":"2024-04-07T11:11:45.690196Z","shell.execute_reply":"2024-04-07T11:11:45.701305Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n\n# # Create a dataset from the loaded data\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [x[\"document\"] for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n})\n\ndel data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:11:45.703032Z","iopub.execute_input":"2024-04-07T11:11:45.703298Z","iopub.status.idle":"2024-04-07T11:11:46.047368Z","shell.execute_reply.started":"2024-04-07T11:11:45.703275Z","shell.execute_reply":"2024-04-07T11:11:46.046460Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}]},{"cell_type":"code","source":"first_model_path = list(model_paths.keys())[0]\ntokenizer = AutoTokenizer.from_pretrained(first_model_path)\n\n# Tokenize the dataset using the 'tokenize' function in parallel\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc = 1)\n\n\nall_preds = []\n# Calculate the total weight\ntotal_weight = sum(model_paths.values())\n\nfor model_path, weight in model_paths.items():\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model = AutoModelForTokenClassification.from_pretrained(model_path)\n    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of = 32)\n    args = TrainingArguments(\n        \".\", \n        per_device_eval_batch_size=1, \n        report_to=\"none\",\n    )\n    trainer = Trainer(\n        model=model, \n        args=args, \n        data_collator=collator, \n        tokenizer=tokenizer,\n    )\n    predictions = trainer.predict(ds).predictions\n    # This idea from this notebook: https://www.kaggle.com/code/olyatsimboy/912-blending-0-903-0-854-deberta3base\n    weighted_predictions = softmax(predictions, axis = -1) * weight\n    all_preds.append(weighted_predictions)\n    del model, trainer, collator, tokenizer, args, predictions, weighted_predictions\n    torch.cuda.empty_cache()\n    gc.collect()\n\n# Calculate the weighted average of predictions\nweighted_average_predictions = np.sum(all_preds, axis=0) / total_weight\n\ndel all_preds, total_weight\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:11:46.048563Z","iopub.execute_input":"2024-04-07T11:11:46.048822Z","iopub.status.idle":"2024-04-07T11:13:10.067400Z","shell.execute_reply.started":"2024-04-07T11:11:46.048799Z","shell.execute_reply":"2024-04-07T11:13:10.066386Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"791e7e9c4c4549df9fbd10331573283e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"config = json.load(open(Path(model_path) / \"config.json\"))\nid2label = config[\"id2label\"]\npreds = weighted_average_predictions.argmax(-1)\npreds_without_O = weighted_average_predictions[:,:,:12].argmax(-1)\nO_preds = weighted_average_predictions[:,:,12]\npreds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\ndel weighted_average_predictions, preds, preds_without_O, O_preds\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:13:10.068502Z","iopub.execute_input":"2024-04-07T11:13:10.068773Z","iopub.status.idle":"2024-04-07T11:13:10.371976Z","shell.execute_reply.started":"2024-04-07T11:13:10.068750Z","shell.execute_reply":"2024-04-07T11:13:10.371057Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"triplets = []\npairs = set()  # membership operation using set is faster O(1) than that of list O(n)\nprocessed = []\nemails = []\nphone_nums = []\nurls = []\nstreets = []\n\n# For each prediction, token mapping, offsets, tokens, and document in the dataset\nfor p, token_map, offsets, tokens, doc, full_text in zip(\n    preds_final, \n    ds[\"token_map\"], \n    ds[\"offset_mapping\"], \n    ds[\"tokens\"], \n    ds[\"document\"],\n    ds[\"full_text\"]\n):\n\n    # Iterate through each token prediction and its corresponding offsets\n    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n        label_pred = id2label[str(token_pred)]  # Predicted label from token\n        if start_idx + end_idx == 0:\n            continue\n        if token_map[start_idx] == -1:\n            start_idx += 1\n        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n            start_idx += 1\n        if start_idx >= len(token_map):\n            break\n        token_id = token_map[start_idx]  # Token ID at start index\n        if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n            continue\n        pair = (doc, token_id)\n        if pair not in pairs:\n            processed.append({\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]})\n            pairs.add(pair)\n    \n    # email\n    for token_idx, token in enumerate(tokens):\n        if re.fullmatch(email_regex, token) is not None:\n            emails.append(\n                {\"document\": doc, \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n            )\n                \n    # phone number\n    matches = phone_num_regex.findall(full_text)\n    if not matches:\n        continue\n    for match in matches:\n        target = [t.text for t in nlp.tokenizer(match)]\n        matched_spans = find_span(target, tokens)\n    for matched_span in matched_spans:\n        for intermediate, token_idx in enumerate(matched_span):\n            prefix = \"I\" if intermediate else \"B\"\n            phone_nums.append(\n                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": tokens[token_idx]}\n            )\n    \n    # url\n    matches = url_regex.findall(full_text)\n    if not matches:\n        continue\n    for match in matches:\n        target = [t.text for t in nlp.tokenizer(match)]\n        matched_spans = find_span(target, tokens)\n    for matched_span in matched_spans:\n        for intermediate, token_idx in enumerate(matched_span):\n            prefix = \"I\" if intermediate else \"B\"\n            urls.append(\n                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-URL_PERSONAL\", \"token_str\": tokens[token_idx]}\n            )\n    \n    # street\n#     matches = street_regex.findall(full_text)\n#     if not matches:\n#         continue\n#     for match in matches:\n#         target = [t.text for t in nlp.tokenizer(match)]\n#         matched_spans = find_span(target, tokens)\n#     for matched_span in matched_spans:\n#         for intermediate, token_idx in enumerate(matched_span):\n#             prefix = \"I\" if intermediate else \"B\"\n#             streets.append(\n#                 {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-STREET_ADDRESS\", \"token_str\": tokens[token_idx]}\n#             )\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:13:10.373381Z","iopub.execute_input":"2024-04-07T11:13:10.373691Z","iopub.status.idle":"2024-04-07T11:13:10.491127Z","shell.execute_reply.started":"2024-04-07T11:13:10.373665Z","shell.execute_reply":"2024-04-07T11:13:10.490338Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(processed + phone_nums + emails + urls)\n\n# Assign each row a unique 'row_id'\ndf[\"row_id\"] = list(range(len(df)))\n\n# Cast your findings into a CSV file for further exploration\ndf[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:13:10.492286Z","iopub.execute_input":"2024-04-07T11:13:10.493053Z","iopub.status.idle":"2024-04-07T11:13:10.523289Z","shell.execute_reply.started":"2024-04-07T11:13:10.493018Z","shell.execute_reply":"2024-04-07T11:13:10.522463Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"    document  token           label   token_str  row_id\n0          7      9  B-NAME_STUDENT    Nathalie       0\n1          7     10  I-NAME_STUDENT       Sylla       1\n2          7    482  B-NAME_STUDENT    Nathalie       2\n3          7    483  I-NAME_STUDENT       Sylla       3\n4          7    741  B-NAME_STUDENT    Nathalie       4\n5          7    742  I-NAME_STUDENT       Sylla       5\n6         10      0  B-NAME_STUDENT       Diego       6\n7         10      1  I-NAME_STUDENT     Estrada       7\n8         10    464  B-NAME_STUDENT       Diego       8\n9         10    465  I-NAME_STUDENT     Estrada       9\n10        16      4  B-NAME_STUDENT    Gilberto      10\n11        16      5  I-NAME_STUDENT      Gamboa      11\n12        20      5  B-NAME_STUDENT       Sindy      12\n13        20      6  I-NAME_STUDENT      Samaca      13\n14        56     12  B-NAME_STUDENT      Nadine      14\n15        56     13  I-NAME_STUDENT        Born      15\n16        86      6  B-NAME_STUDENT      Eladio      16\n17        86      7  I-NAME_STUDENT       Amaya      17\n18        93      0  B-NAME_STUDENT      Silvia      18\n19        93      1  I-NAME_STUDENT  Villalobos      19\n20       104      7  B-NAME_STUDENT          Dr      20\n21       104      8  B-NAME_STUDENT       Sakir      21\n22       104      9  I-NAME_STUDENT       Ahmad      22\n23       112      5  B-NAME_STUDENT   Francisco      23\n24       112      6  I-NAME_STUDENT    Ferreira      24\n25       123     32  B-NAME_STUDENT     Stefano      25\n26       123     33  I-NAME_STUDENT      Lovato      26","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>token</th>\n      <th>label</th>\n      <th>token_str</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>9</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>10</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>482</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>483</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>741</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7</td>\n      <td>742</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10</td>\n      <td>464</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>465</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>16</td>\n      <td>4</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Gilberto</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>16</td>\n      <td>5</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Gamboa</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>20</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sindy</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>20</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Samaca</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>56</td>\n      <td>12</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nadine</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>56</td>\n      <td>13</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Born</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>86</td>\n      <td>6</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Eladio</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>86</td>\n      <td>7</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Amaya</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>93</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Silvia</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>93</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Villalobos</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>104</td>\n      <td>7</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Dr</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>104</td>\n      <td>8</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sakir</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>104</td>\n      <td>9</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ahmad</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>112</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Francisco</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>112</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ferreira</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>123</td>\n      <td>32</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Stefano</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>123</td>\n      <td>33</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Lovato</td>\n      <td>26</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}